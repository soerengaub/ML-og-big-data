{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "# Suppress specific warnings from scikit-learn\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a simple dataset\n",
    "data = {\n",
    "    'X': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'y': [1.3, 3.5, 4.2, 5.0, 6.8, 7.4, 8.0, 8.4, 9.6, 10.1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X = df[['X']]\n",
    "y = df['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting X against y to show the relationship\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(df['X'], df['y'], color='blue')\n",
    "plt.title('Relationship between X and y')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pictures/dt-xy.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and training the model\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def plot_tree_and_predictions(depth, X_train, y_train, df):\n",
    "    # Train the model\n",
    "    regressor = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions for a range of X values\n",
    "    X_range = np.linspace(df['X'].min(), df['X'].max(), 100).reshape(-1, 1)\n",
    "    y_pred = regressor.predict(X_range)\n",
    "\n",
    "    # Set up a figure with two subplots with different widths\n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    gs = GridSpec(1, 2, width_ratios=[3, 1])  # Adjust the ratio here\n",
    "\n",
    "    # Plotting the decision tree\n",
    "    ax0 = fig.add_subplot(gs[0])\n",
    "    tree.plot_tree(regressor, filled=True, feature_names=['X'], rounded=True, ax=ax0)\n",
    "    ax0.set_title(f'Decision Tree with max_depth = {depth}')\n",
    "\n",
    "    # Plotting the original data and the predictions\n",
    "    ax1 = fig.add_subplot(gs[1])\n",
    "    ax1.scatter(df['X'], df['y'], color='blue', label='Original Data')\n",
    "    ax1.plot(X_range, y_pred, color='red', label='Tree Predictions')\n",
    "    ax1.set_title(f'Decision Tree Predictions with max_depth = {depth}')\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('y')\n",
    "\n",
    "    # Drawing vertical lines for the split points\n",
    "    # Extracting split points from the decision tree\n",
    "    split_points = regressor.tree_.threshold\n",
    "    split_points = split_points[split_points != -2]  # Filter out non-split nodes\n",
    "    for sp in split_points:\n",
    "        ax1.axvline(x=sp, color='green', linestyle='--')\n",
    "\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'pictures/dt-depth-{depth}.pdf')\n",
    "    plt.show()\n",
    "\n",
    "# Plotting both the tree and its predictions for different depths\n",
    "for depth in [1, 2, 3]:\n",
    "    plot_tree_and_predictions(depth, X_train, y_train, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualizing the entire regression tree\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# tree.plot_tree(regressor, filled=True, feature_names=['X'], rounded=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "\n",
    "# Creating a more complex synthetic dataset for classification\n",
    "# X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, \n",
    "#                                            n_informative=2, n_clusters_per_class=2, \n",
    "#                                            random_state=42)\n",
    "\n",
    "# X, y = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Visualizing the more complex 2D dataset\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title(\"Synthetic 2D Classification Dataset\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.colorbar(label='Class')\n",
    "plt.savefig('pictures/moon_dataset.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def plot_classification_tree_and_predictions_2D(depth, X, y):\n",
    "    # Train the model\n",
    "    classifier = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    classifier.fit(X, y)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    y_pred = classifier.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "    # Set up a figure with two subplots with different widths\n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    gs = GridSpec(1, 2, width_ratios=[3, 1.5])  # Adjust the ratio here\n",
    "\n",
    "    # Plotting the decision tree\n",
    "    ax0 = fig.add_subplot(gs[0])\n",
    "    plot_tree(classifier, filled=True, ax=ax0, class_names=['0', '1'])\n",
    "    ax0.set_title(f'Decision Tree with max_depth = {depth}')\n",
    "\n",
    "    # Plotting the decision boundaries and split points\n",
    "    ax1 = fig.add_subplot(gs[1])\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), \n",
    "                         np.linspace(y_min, y_max, 500))\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Contour plot for decision boundaries\n",
    "    ax1.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "\n",
    "    # Scatter plot of actual data points\n",
    "    ax1.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k', cmap='viridis')\n",
    "\n",
    "    # Plotting the splits\n",
    "    tree_ = classifier.tree_\n",
    "    feature = tree_.feature\n",
    "    threshold = tree_.threshold\n",
    "\n",
    "    for i in range(tree_.node_count):\n",
    "        if feature[i] != -2:  # Not a leaf node\n",
    "            if feature[i] == 0:  # Split on feature 1\n",
    "                ax1.axvline(x=threshold[i], color='green', linestyle='--')\n",
    "            elif feature[i] == 1:  # Split on feature 2\n",
    "                ax1.axhline(y=threshold[i], color='green', linestyle='--')\n",
    "\n",
    "    # Title with accuracy\n",
    "    ax1.set_title(f'Decision Boundaries (Accuracy: {accuracy:.2f})')\n",
    "    ax1.set_xlabel('X[0]')\n",
    "    ax1.set_ylabel('X[1]')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'pictures/dt-class-moon-depth-{depth}.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for depth in [1, 2, 3, 4]:\n",
    "    plot_classification_tree_and_predictions_2D(depth, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Heart Disease dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "column_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
    "data = pd.read_csv(url, names=column_names, na_values=\"?\")\n",
    "data = data.dropna()\n",
    "\n",
    "# Prepare the features and labels\n",
    "X = data.drop('target', axis=1).values\n",
    "y = data['target'].values\n",
    "\n",
    "# Convert labels to binary (-1, 1)\n",
    "y = np.where(y > 0, 1, -1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "class MyRandomForestClassifier:\n",
    "    def __init__(self, n_estimators=10, max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        for _ in range(self.n_estimators):\n",
    "            row_indices = ?\n",
    "            n_selected_features = ?\n",
    "            feature_indices = ?\n",
    "            X_sample, y_sample = X[row_indices][:, feature_indices], y[row_indices]\n",
    "            tree = ?\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append((tree, feature_indices))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array(\n",
    "            [?\n",
    "             for tree, features in self.trees]\n",
    "        ).T\n",
    "        return mode(predictions, axis=1).mode.ravel()\n",
    "    \n",
    "    def feature_importances(self):\n",
    "        # Initialize an array to store feature importances\n",
    "        importances = np.zeros(X_train.shape[1])\n",
    "\n",
    "        # Sum up feature importances from each tree\n",
    "        for tree, feature_indices in self.trees:\n",
    "            tree_importances = tree.feature_importances_\n",
    "            for i, idx in enumerate(feature_indices):\n",
    "                importances[idx] += tree_importances[i]\n",
    "\n",
    "        # Average the importances over all trees\n",
    "        importances /= self.n_estimators\n",
    "\n",
    "        return importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a single decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=5)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_pred = tree_clf.predict(X_test)\n",
    "tree_accuracy = accuracy_score(y_test, tree_pred)\n",
    "\n",
    "# Train your custom random forest classifier\n",
    "forest_clf = MyRandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "forest_pred = forest_clf.predict(X_test)\n",
    "forest_accuracy = accuracy_score(y_test, forest_pred)\n",
    "\n",
    "# Compare their accuracies\n",
    "print(\"Decision Tree Accuracy:\", tree_accuracy)\n",
    "print(\"Random Forest Accuracy:\", forest_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_forest = confusion_matrix(y_test, forest_pred)\n",
    "cm_tree = confusion_matrix(y_test, tree_pred)\n",
    "\n",
    "# Plotting the confusion matrices\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Confusion matrix for the decision tree\n",
    "sns.heatmap(cm_tree, annot=True, fmt='d', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title(f'Decision Tree Accuracy: {tree_accuracy:.3f}')\n",
    "ax[0].set_xlabel('Predicted Label')\n",
    "ax[0].set_ylabel('True Label')\n",
    "\n",
    "# Confusion matrix for the random forest\n",
    "sns.heatmap(cm_forest, annot=True, fmt='d', cmap='Blues', ax=ax[1])\n",
    "ax[1].set_title(f'Random Forest Accuracy: {forest_accuracy:.3f}')\n",
    "ax[1].set_xlabel('Predicted Label')\n",
    "ax[1].set_ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pictures/dt_vs_rf_confusion_matrix.pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.feature_importances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming forest_clf.feature_importances() and column_names are defined in your environment\n",
    "feature_importances = forest_clf.feature_importances()\n",
    "feature_importances /= feature_importances.max() \n",
    "feature_importances *= 100\n",
    "\n",
    "feature_names = column_names[:-1]  # Excluding the target variable name\n",
    "\n",
    "# Sorting the features by their importance in ascending order\n",
    "sorted_indices = np.argsort(feature_importances)\n",
    "\n",
    "# To reverse the order to descending\n",
    "sorted_indices_desc = sorted_indices[::-1]\n",
    "\n",
    "# Creating a horizontal bar chart with black borders around the bars\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.title(\"Importances in My Random Forest\", fontsize=18)\n",
    "bars = plt.barh(range(len(sorted_indices_desc)), feature_importances[sorted_indices_desc], align='center', color='red', edgecolor='black')\n",
    "plt.yticks(range(len(sorted_indices_desc)), np.array(feature_names)[sorted_indices_desc], fontsize=14)\n",
    "plt.xlabel(\"Variable Importance\", fontsize=16)\n",
    "\n",
    "# Customize the plot borders\n",
    "ax = plt.gca()  # Get the current Axes instance\n",
    "ax.spines['top'].set_visible(False)    # Hide the top spine\n",
    "ax.spines['right'].set_visible(False)  # Hide the right spine\n",
    "ax.spines['left'].set_visible(False)   # Hide the left spine\n",
    "# Optionally, you can make the bottom spine more prominent\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('pictures/rf_feature_importance.pdf')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
